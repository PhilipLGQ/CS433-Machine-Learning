{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import files to use in preprocessing and machine learning\n",
    "from implementations import *\n",
    "from proj1_helpers import *\n",
    "from preprocess import *\n",
    "from cross_validation import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Download train data and supply path here \n",
    "DATA_TRAIN_PATH = '../data/train.csv' \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([100000, 100001, 100002, ..., 349997, 349998, 349999])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000,)\n",
      "(250000, 30)\n",
      "(250000,)\n"
     ]
    }
   ],
   "source": [
    "# Check the array shape of y, tX, and ids\n",
    "print(y.shape)\n",
    "print(tX.shape)\n",
    "print(ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([100003, 100004, 100008, ..., 349966, 349992, 349993])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reordered_id = reorder_id(tX, ids)\n",
    "reordered_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In observing the original training data, we found out that there exists missing data all over tX. The missing data are represented as value -999. Considering that these columns are critical in model training, we cannot simply delete these rows with -999 values. Therefore, we need to process the original training set before model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we check the columns of tX to obtain an overview of missing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 0 has 15.2456% percentage of missing values\n",
      "P(y = 1|x having -999) = 7.438%\n",
      "P(y = -1|x having -999) = 92.562% \n",
      "\n",
      "Column 4 has 70.9828% percentage of missing values\n",
      "P(y = 1|x having -999) = 29.980%\n",
      "P(y = -1|x having -999) = 70.020% \n",
      "\n",
      "Column 5 has 70.9828% percentage of missing values\n",
      "P(y = 1|x having -999) = 29.980%\n",
      "P(y = -1|x having -999) = 70.020% \n",
      "\n",
      "Column 6 has 70.9828% percentage of missing values\n",
      "P(y = 1|x having -999) = 29.980%\n",
      "P(y = -1|x having -999) = 70.020% \n",
      "\n",
      "Column 12 has 70.9828% percentage of missing values\n",
      "P(y = 1|x having -999) = 29.980%\n",
      "P(y = -1|x having -999) = 70.020% \n",
      "\n",
      "Column 23 has 39.9652% percentage of missing values\n",
      "P(y = 1|x having -999) = 25.514%\n",
      "P(y = -1|x having -999) = 74.486% \n",
      "\n",
      "Column 24 has 39.9652% percentage of missing values\n",
      "P(y = 1|x having -999) = 25.514%\n",
      "P(y = -1|x having -999) = 74.486% \n",
      "\n",
      "Column 25 has 39.9652% percentage of missing values\n",
      "P(y = 1|x having -999) = 25.514%\n",
      "P(y = -1|x having -999) = 74.486% \n",
      "\n",
      "Column 26 has 70.9828% percentage of missing values\n",
      "P(y = 1|x having -999) = 29.980%\n",
      "P(y = -1|x having -999) = 70.020% \n",
      "\n",
      "Column 27 has 70.9828% percentage of missing values\n",
      "P(y = 1|x having -999) = 29.980%\n",
      "P(y = -1|x having -999) = 70.020% \n",
      "\n",
      "Column 28 has 70.9828% percentage of missing values\n",
      "P(y = 1|x having -999) = 29.980%\n",
      "P(y = -1|x having -999) = 70.020% \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check whether the missing values are associated with the classification result\n",
    "for col in range(tX.shape[1]):\n",
    "    tX_T = np.transpose(tX)\n",
    "    \n",
    "    null = (tX_T[col] == -999)\n",
    "    null_s = np.logical_and(y >= 0, null)\n",
    "    null_b = np.logical_and(y < 0, null)\n",
    "    \n",
    "    tX_null = tX[null]\n",
    "    tX_null_s = tX[null_s]\n",
    "    tX_null_b = tX[null_b]\n",
    "    \n",
    "    if (tX_null.shape[0] > 0):\n",
    "        # Print the percentage of column 'col' having a -999 (missing) value\n",
    "        print('Column', col, 'has {}% percentage of missing values'.format(tX_null.shape[0] * 100 / tX.shape[0]))\n",
    "\n",
    "        # Print the conditional probability of P(y = 1|x having -999)\n",
    "        print('P(y = 1|x having -999) = {:.3f}%'.format(tX_null_s.shape[0] * 100 / tX_null.shape[0]))\n",
    "        \n",
    "        # Print the conditional probability of P(y = -1|x having -999)\n",
    "        print('P(y = -1|x having -999) = {:.3f}% \\n'.format(tX_null_b.shape[0] * 100 / tX_null.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that 11 columns contains at least one -999 (missing value). Now we check whether some of the missing values are dependent on the column named 'PRI_jet_num' (column No. 23), since 'PRI_jet_num' has a discrete value range {0, 1, 2, 3} and our observation on the beginning data rows showed a dependency of some missing values to the value of 'PRI_jet_num' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of rows for different PRI_jet_num: 250000 \n",
      "\n",
      "PRI_jet_num = 0 No. of columns having -999 (a missing value):[10, 11]\n",
      "PRI_jet_num = 1 No. of columns having -999 (a missing value):[8, 7]\n",
      "PRI_jet_num = 2 No. of columns having -999 (a missing value):[0, 1]\n",
      "PRI_jet_num = 3 No. of columns having -999 (a missing value):[0, 1]\n"
     ]
    }
   ],
   "source": [
    "PRI_jet_range = [i for i in range(0, 4)]\n",
    "PRI_jet_sum = []\n",
    "PRI_jet_null = []\n",
    "\n",
    "for value in PRI_jet_range:\n",
    "    tX_PRI = tX[tX[:, 22] == value]\n",
    "    \n",
    "    # Append values of row numbers for different PRI_jet_num, finally sum up to see whether it equals to the length of tX\n",
    "    PRI_jet_sum.append(len(tX_PRI))\n",
    "    \n",
    "    # Count the number of missing columns corresponding to different PRI_jet_num values\n",
    "    PRI_jet_keys = []\n",
    "    for i in range (len(tX_PRI)):\n",
    "        tX_null_cols = np.count_nonzero(tX_PRI[i] == -999, axis = 0)\n",
    "        PRI_jet_keys.append(tX_null_cols)\n",
    "    \n",
    "    PRI_jet_null.append(list(set(PRI_jet_keys)))\n",
    "\n",
    "    \n",
    "print(\"Sum of rows for different PRI_jet_num: {} \\n\".format(sum(PRI_jet_sum)))\n",
    "\n",
    "for i in range(4):\n",
    "    print(\"PRI_jet_num =\", PRI_jet_range[i], \"No. of columns having -999 (a missing value):{}\".format(PRI_jet_null[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above analysis showed that one column with -999 (missing value) is independent of the column 'PRI_jet_num', we check the original training set and we can easily find out that the first tX column 'DER_mass_MMC' is independent of 'PRI_jet_num'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the data analysis above, we conduct the following method to pre-process the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set H-parameters\n",
    "K_FOLD = 10\n",
    "DEGREE = np.arange(2, 8)\n",
    "SEED = 5\n",
    "LAMBDA = np.logspace(-8, -1, 50)\n",
    "\n",
    "FILEHANDLE = open('result.txt','a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal(x, y, degrees, k_fold, lambdas, seed=1):\n",
    "    # Split the data into k-fold\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    \n",
    "    # Set lists for collecting best lambda & rmse for each degree\n",
    "    best_lambda = []\n",
    "    best_rmse = []\n",
    "    \n",
    "    for degree in degrees:\n",
    "        rmse_val = []\n",
    "        \n",
    "        for lambda_ in lambdas:\n",
    "            rmse_val_lambda_ = []\n",
    "            \n",
    "            for k in range(k_fold):\n",
    "                _, loss_val, w = cross_validation(y, x, k_indices, k, lambda_, degree)\n",
    "                rmse_val_lambda_.append(loss_val)\n",
    "                \n",
    "            FILEHANDLE.write(\"lambda {}\\n\".format(lambda_))\n",
    "            FILEHANDLE.write(\"loss {}\\n\".format(np.mean(rmse_val_lambda_)))\n",
    "            FILEHANDLE.write(\"degree {}\\n\".format(degree))\n",
    "            FILEHANDLE.write(\"\\n\")\n",
    "\n",
    "            rmse_val.append(np.mean(rmse_val_lambda_))\n",
    "        \n",
    "        index_opt_lambda = np.argmin(rmse_val)\n",
    "        best_lambda.append(lambdas[index_opt_lambda])\n",
    "        best_rmse.append(rmse_val[index_opt_lambda])\n",
    "    \n",
    "    opt_degree = degrees[np.argmin(best_rmse)]\n",
    "    opt_lambda = best_lambda[np.argmin(best_rmse)]\n",
    "    \n",
    "    return opt_degree, opt_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(feature, label, degrees, k_fold, lambdas):\n",
    "    opt_degree, opt_lambda = [], []\n",
    "    \n",
    "    feature_arr, label_arr = split_reformat_train(feature, label)\n",
    "    \n",
    "    # Parallel iteration for cross validation to select the best degree of complexity and learning rate\n",
    "    for f, l in zip(feature_arr, label_arr):\n",
    "\n",
    "        opt_d, opt_l = find_optimal(f, l, degrees, k_fold, lambdas)        \n",
    "        opt_degree.append(opt_d)\n",
    "        opt_lambda.append(opt_l)\n",
    "        \n",
    "        FILEHANDLE.write(\"Optimal degree under the experiment {}\\n\".format(opt_degree))\n",
    "        FILEHANDLE.write(\"Optimal learning rate under the experiment {}\\n\".format(opt_lambda))\n",
    "        FILEHANDLE.write(\"\\n\\n\\n\")\n",
    "        \n",
    "    FILEHANDLE.close()\n",
    "        \n",
    "    return opt_degree, opt_lambda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_weights(feature, label, best_d, best_l):\n",
    "    \n",
    "    weights = []\n",
    "    \n",
    "    feature_arr = split_reformat_feature(feature)\n",
    "    label_arr = split_label(feature, label)\n",
    "    \n",
    "    for idx, (f, l) in enumerate(zip(feature_arr, label_arr)):\n",
    "        # Polynomial Feature Transform\n",
    "        poly_feature = build_poly(f, best_d[idx])\n",
    "    \n",
    "        # Training ridge regression on the entire training \n",
    "        w, _ = ridge_regression(l, poly_feature, best_l[idx])\n",
    "        weights.append(w)\n",
    "        \n",
    "    return weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_data_pred(feature, label, _ids, best_d, best_l):\n",
    "    pred = []\n",
    "    \n",
    "    feature_arr = split_reformat_feature(feature)\n",
    "    label_arr = split_label(feature, label)\n",
    "    weights = generate_weights(feature, label, best_degree, best_lambda)\n",
    "    \n",
    "    for idx, w in enumerate(weights):\n",
    "        poly_feature = build_poly(feature_arr[idx], best_d[idx])\n",
    "        pred.extend(predict_labels(w, poly_feature))\n",
    "    \n",
    "    return pred\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat_result(pred, re_ids):\n",
    "    pred_pair = [(i, j) for i, j in zip(re_ids, pred)]\n",
    "    result = [j for _, j in sorted(pred_pair)]\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_data_pred(feature, _ids, weights, best_d):\n",
    "    pred = []\n",
    "    \n",
    "    feature_arr = split_reformat_test(feature)\n",
    "    reordered_ids = reorder_id(feature, _ids)\n",
    "    \n",
    "    for idx, (f,w) in enumerate(zip(feature_arr, weights)):\n",
    "        poly_feature = build_poly(f, best_d[idx])\n",
    "        pred.extend(predict_labels(w, poly_feature))\n",
    "                    \n",
    "    return reformat_result(pred, reordered_ids)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_degree, best_lambda = train_models(tX, y, DEGREE, K_FOLD, LAMBDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_degree = [2, 6, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_lambda = [0.0014873521072935117, 0.01, 0.007278953843983146]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.801508"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = train_data_pred(tX, y, ids, best_degree, best_lambda)\n",
    "\n",
    "sum(reformat_result(p, reordered_id) == y) / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 6, 5]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0011721022975334817, 0.018873918221350997, 0.006210169418915616]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = generate_weights(tX, y, best_degree, best_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'data/pred.csv'\n",
    "y_pred = test_data_pred(tX_test, ids_test, w, best_degree)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# OUTPUT_PATH = 'data/pred.csv' # TODO: fill in desired name of output file for submission\n",
    "# y_pred = predict_labels(weights, tX_test)\n",
    "# create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
