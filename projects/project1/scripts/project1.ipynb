{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import files to use in preprocessing and machine learning\n",
    "from implementations import *\n",
    "from proj1_helpers import *\n",
    "from preprocess import *\n",
    "from cross_validation import *\n",
    "from helpers import *\n",
    "from costs import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Download train data and supply path here \n",
    "DATA_TRAIN_PATH = 'data/train.csv' \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000,)\n",
      "(250000, 30)\n",
      "(250000,)\n"
     ]
    }
   ],
   "source": [
    "# Check the array shape of y, tX, and ids\n",
    "print(y.shape)\n",
    "print(tX.shape)\n",
    "print(ids.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In observing the original training data, we found out that there exists missing data all over tX. The missing data are represented as value -999. Considering that these columns are critical in model training, we cannot simply delete these rows with -999 values. Therefore, we need to process the original training set before model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we check the columns of tX to obtain an overview of missing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether the missing values are associated with the classification result\n",
    "for col in range(tX.shape[1]):\n",
    "    tX_T = np.transpose(tX)\n",
    "    \n",
    "    null = (tX_T[col] == -999)\n",
    "    null_s = np.logical_and(y >= 0, null)\n",
    "    null_b = np.logical_and(y < 0, null)\n",
    "    \n",
    "    tX_null = tX[null]\n",
    "    tX_null_s = tX[null_s]\n",
    "    tX_null_b = tX[null_b]\n",
    "    \n",
    "    if (tX_null.shape[0] > 0):\n",
    "        # Print the percentage of column 'col' having a -999 (missing) value\n",
    "        print('Column', col, 'has {}% percentage of missing values'.format(tX_null.shape[0] * 100 / tX.shape[0]))\n",
    "\n",
    "        # Print the conditional probability of P(y = 1|x having -999)\n",
    "        print('P(y = 1|x having -999) = {:.3f}%'.format(tX_null_s.shape[0] * 100 / tX_null.shape[0]))\n",
    "        \n",
    "        # Print the conditional probability of P(y = -1|x having -999)\n",
    "        print('P(y = -1|x having -999) = {:.3f}% \\n'.format(tX_null_b.shape[0] * 100 / tX_null.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that 11 columns contains at least one -999 (missing value). Now we check whether some of the missing values are dependent on the column named 'PRI_jet_num' (column No. 23), since 'PRI_jet_num' has a discrete value range {0, 1, 2, 3} and our observation on the beginning data rows showed a dependency of some missing values to the value of 'PRI_jet_num' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRI_jet_range = [i for i in range(0, 4)]\n",
    "PRI_jet_sum = []\n",
    "PRI_jet_null = []\n",
    "\n",
    "for value in PRI_jet_range:\n",
    "    tX_PRI = tX[tX[:, 22] == value]\n",
    "    \n",
    "    # Append values of row numbers for different PRI_jet_num, finally sum up to see whether it equals to the length of tX\n",
    "    PRI_jet_sum.append(len(tX_PRI))\n",
    "    \n",
    "    # Count the number of missing columns corresponding to different PRI_jet_num values\n",
    "    PRI_jet_keys = []\n",
    "    for i in range (len(tX_PRI)):\n",
    "        tX_null_cols = np.count_nonzero(tX_PRI[i] == -999, axis = 0)\n",
    "        PRI_jet_keys.append(tX_null_cols)\n",
    "    \n",
    "    PRI_jet_null.append(list(set(PRI_jet_keys)))\n",
    "\n",
    "    \n",
    "print(\"Sum of rows for different PRI_jet_num: {} \\n\".format(sum(PRI_jet_sum)))\n",
    "\n",
    "for i in range(4):\n",
    "    print(\"PRI_jet_num =\", PRI_jet_range[i], \"No. of columns having -999 (a missing value):{}\".format(PRI_jet_null[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above analysis showed that one column with -999 (missing value) is independent of the column 'PRI_jet_num', we check the original training set and we can easily find out that the first tX column 'DER_mass_MMC' is independent of 'PRI_jet_num'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the data analysis above, we conduct the following method to pre-process the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99913, 18)\n",
      "(77544, 22)\n",
      "(72543, 29)\n"
     ]
    }
   ],
   "source": [
    "TX, Y, r_ids = data_preprocess(tX, y, ids, replacing='lr', mode='std_norm')\n",
    "\n",
    "print(TX[0].shape)\n",
    "print(TX[1].shape)\n",
    "print(TX[2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Not used)\n",
    "# Split the database based on 'PRI_jet_num' (0, 1, 2&3)\n",
    "tX_jet_0, tX_jet_1, tX_jet_2, y_jet_0, y_jet_1, y_jet_2, r_ids = split_reformat_data(tX, y, ids)\n",
    "\n",
    "#print(tX_jet_0.shape)\n",
    "#print(tX_jet_1.shape)\n",
    "#print(tX_jet_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Not used)\n",
    "# Replacing the missing values of first feature column\n",
    "# 'median', 'mean', or 'lr' (linear regression)\n",
    "replace_missing_value(tX_jet_0, 0, 'lr')\n",
    "replace_missing_value(tX_jet_1, 0, 'lr')\n",
    "replace_missing_value(tX_jet_2, 0, 'lr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Not used)\n",
    "# (Use the above block or this block)\n",
    "# Replacing the missing values by k-means clustering\n",
    "k_means_replacing(tX_jet_0)\n",
    "k_means_replacing(tX_jet_1)\n",
    "k_means_replacing(tX_jet_23)\n",
    "\n",
    "print((tX_jet_0[tX_jet_0[:, 0]== -999][:,0]).shape)\n",
    "print((tX_jet_1[tX_jet_1[:, 0]== -999][:,0]).shape)\n",
    "print((tX_jet_23[tX_jet_23[:, 0]== -999][:,0]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Not used)\n",
    "# Standardize and/or normalize the splitted training data\n",
    "tx_0 = std_norm_preprocess(tx_jet_0, 'std_norm')\n",
    "tx_1 = std_norm_preprocess(tx_jet_1, 'std_norm')\n",
    "tx_2 = std_norm_preprocess(tx_jet_2, 'std_norm')\n",
    "\n",
    "TX = [tx_0, tx_1, tx_2]\n",
    "Y = [y_jet_0, y_jet_1, y_jet_2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression with Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set H-parameters\n",
    "GAMMA_LRGD = 0.0005\n",
    "MAX_ITERS_LRGD = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 74.4 GiB for an array with shape (99913, 99913) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2676/1702583946.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mINITIAL_W\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mw_lrgd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_lrgd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mleast_squares_GD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mINITIAL_W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_ITERS_LRGD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGAMMA_LRGD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss of tx_{},'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_lrgd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'iterations: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMAX_ITERS_LRGD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/CS433_git/projects/project1/scripts/implementations.py\u001b[0m in \u001b[0;36mleast_squares_GD\u001b[0;34m(y, tx, initial_w, max_iters, gamma)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mn_iter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_loss_mse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/CS433_git/projects/project1/scripts/helpers.py\u001b[0m in \u001b[0;36mcompute_gradient\u001b[0;34m(y, tx, w)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompute_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 74.4 GiB for an array with shape (99913, 99913) and data type float64"
     ]
    }
   ],
   "source": [
    "INITIAL_W = np.zeros((TX[0].shape[1]+1, 1))\n",
    "w_lrgd, loss_lrgd = least_squares_GD(Y[0], TX[0], INITIAL_W, MAX_ITERS_LRGD, GAMMA_LRGD)\n",
    "    \n",
    "print('Loss of tx_{},'.format(i), loss_lrgd, 'iterations: {}'.format(MAX_ITERS_LRGD))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression with Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set H-parameters\n",
    "GAMMA_LRSGD = 0.0005\n",
    "MAX_ITERS_LRSGD = 200\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WEIGHT_LRSGD = []\n",
    "#LOSS_LRSGD = []\n",
    "print('Parameters for training:\\n')\n",
    "print('Gamma = {}'.format(GAMMA_LRSGD))\n",
    "print('Iterations = {}'.format(MAX_ITERS_LRSGD))\n",
    "print('Batch size = {}'.format(BATCH_SIZE))\n",
    "\n",
    "# Fine-tuning the data for each splitted training set\n",
    "INITIAL_W = np.zeros((TX[0].shape[1]+1, 1))\n",
    "w_lrsgd, loss_lrsgd = least_squares_SGD(Y[0], TX[0], INITIAL_W, MAX_ITERS_LRSGD, \n",
    "                                          GAMMA_LRSGD, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Parameters for training:\\n')\n",
    "print('Gamma = {}'.format(GAMMA_LRSGD))\n",
    "print('Iterations = {}'.format(MAX_ITERS_LRSGD))\n",
    "print('Batch size = {}'.format(BATCH_SIZE))\n",
    "\n",
    "# Fine-tuning the data for each splitted training set\n",
    "INITIAL_W = np.zeros((TX[1].shape[1]+1, 1))\n",
    "w_lrsgd_1, loss_lrsgd_1 = least_squares_SGD(Y[1], TX[1], INITIAL_W, MAX_ITERS_LRSGD, \n",
    "                                          GAMMA_LRSGD, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Parameters for training:\\n')\n",
    "print('Gamma = {}'.format(GAMMA_LRSGD))\n",
    "print('Iterations = {}'.format(MAX_ITERS_LRSGD))\n",
    "print('Batch size = {}'.format(BATCH_SIZE))\n",
    "\n",
    "# Fine-tuning the data for each splitted training set\n",
    "INITIAL_W = np.zeros((TX[2].shape[1]+1, 1))\n",
    "w_lrsgd_2, loss_lrsgd_2 = least_squares_SGD(Y[2], TX[2], INITIAL_W, MAX_ITERS_LRSGD, \n",
    "                                          GAMMA_LRSGD, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Squares with Normal Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set H-parameters\n",
    "DEGREE = np.arange(2, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Degree=2, Loss=2.716973059046098\n",
      "Degree=3, Loss=0.5977268952756707\n",
      "Degree=4, Loss=0.2446389906141943\n",
      "Degree=5, Loss=0.2412249117818718\n",
      "Degree=6, Loss=0.23841392000305464\n",
      "Degree=7, Loss=0.2687330711970408\n",
      "Degree=8, Loss=0.277579515434394\n",
      "Degree=9, Loss=0.23403774649861855\n",
      "Degree=10, Loss=0.29784256510695334\n",
      "\n",
      "Optimal Degree=9, Loss=0.23403774649861855\n"
     ]
    }
   ],
   "source": [
    "# Fine-tuning each splitted training dataset\n",
    "W_LSNE_0 = []\n",
    "LOSS_LSNE_0 = []\n",
    "\n",
    "for i in DEGREE:\n",
    "    poly_tx = build_poly(TX[0], i)\n",
    "    w_lsne, loss_lsne = least_squares(Y[0], poly_tx)\n",
    "    print('Degree={a}, Loss={b}'.format(a=i, b=loss_lsne))\n",
    "    \n",
    "    W_LSNE_0.append(w_lsne)\n",
    "    LOSS_LSNE_0.append(loss_lsne)\n",
    "\n",
    "# Obtain the optimal training weight\n",
    "w_lsne_opt_0 = W_LSNE_0[np.argmin(LOSS_LSNE_0)]\n",
    "\n",
    "print('\\nOptimal Degree={a}, Loss={b}'.format(a=np.min(DEGREE)+np.argmin(LOSS_LSNE_0),\n",
    "                                            b=LOSS_LSNE_0[np.argmin(LOSS_LSNE_0)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Degree=2, Loss=0.3726320941707011\n",
      "Degree=3, Loss=0.3388298682447978\n",
      "Degree=4, Loss=0.32928504062212327\n",
      "Degree=5, Loss=0.32193768701715275\n",
      "Degree=6, Loss=0.3175146488850505\n",
      "Degree=7, Loss=0.31260805456110285\n",
      "Degree=8, Loss=0.3061163557630781\n",
      "Degree=9, Loss=0.2984343363651026\n",
      "Degree=10, Loss=0.34563275797245163\n",
      "\n",
      "Optimal Degree=9, Loss=0.2984343363651026\n"
     ]
    }
   ],
   "source": [
    "# Fine-tuning each splitted training dataset\n",
    "W_LSNE_1 = []\n",
    "LOSS_LSNE_1 = []\n",
    "\n",
    "for i in DEGREE:\n",
    "    poly_tx = build_poly(TX[1], i)\n",
    "    w_lsne, loss_lsne = least_squares(Y[1], poly_tx)\n",
    "    print('Degree={a}, Loss={b}'.format(a=i, b=loss_lsne))\n",
    "    \n",
    "    W_LSNE_1.append(w_lsne)\n",
    "    LOSS_LSNE_1.append(loss_lsne)\n",
    "\n",
    "# Obtain the optimal training weight\n",
    "w_lsne_opt_1 = W_LSNE_1[np.argmin(LOSS_LSNE_1)]\n",
    "\n",
    "print('\\nOptimal Degree={a}, Loss={b}'.format(a=np.min(DEGREE)+np.argmin(LOSS_LSNE_1),\n",
    "                                            b=LOSS_LSNE_1[np.argmin(LOSS_LSNE_1)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Degree=2, Loss=0.3628695022914208\n",
      "Degree=3, Loss=0.32984694163527467\n",
      "Degree=4, Loss=0.3102700012975416\n",
      "Degree=5, Loss=0.3015283412016949\n",
      "Degree=6, Loss=0.2985635757978061\n",
      "Degree=7, Loss=0.29361965896198283\n",
      "Degree=8, Loss=0.28149784722826043\n",
      "Degree=9, Loss=0.2694621706236164\n",
      "Degree=10, Loss=0.2641948086844093\n",
      "\n",
      "Optimal Degree=10, Loss=0.2641948086844093\n"
     ]
    }
   ],
   "source": [
    "# Fine-tuning each splitted training dataset\n",
    "W_LSNE_2 = []\n",
    "LOSS_LSNE_2 = []\n",
    "\n",
    "for i in DEGREE:\n",
    "    poly_tx = build_poly(TX[2], i)\n",
    "    w_lsne, loss_lsne = least_squares(Y[2], poly_tx)\n",
    "    print('Degree={a}, Loss={b}'.format(a=i, b=loss_lsne))\n",
    "    \n",
    "    W_LSNE_2.append(w_lsne)\n",
    "    LOSS_LSNE_2.append(loss_lsne)\n",
    "\n",
    "# Obtain the optimal training weight\n",
    "w_lsne_opt_2 = W_LSNE_2[np.argmin(LOSS_LSNE_2)]\n",
    "\n",
    "print('\\nOptimal Degree={a}, Loss={b}'.format(a=np.min(DEGREE)+np.argmin(LOSS_LSNE_2),\n",
    "                                            b=LOSS_LSNE_2[np.argmin(LOSS_LSNE_2)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the accuracy on the training set\n",
    "W_LSNE = [w_lsne_opt_0, w_lsne_opt_1, w_lsne_opt_2]\n",
    "pred_tr_lsne = data_pred(TX, W_LSNE, step='tr')\n",
    "accu_lsne = accuracy_pred(pred_tr_lsne, Y, _ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accu_lsne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set H-parameters\n",
    "K_FOLD = 10\n",
    "DEGREE = np.arange(1, 7)\n",
    "SEED = 5\n",
    "LAMBDA = np.logspace(-6, -3, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find an optimal set of polynomial expansion degree and learning rate (lambda)\n",
    "# Then calculate the corresponding weight and loss (rmse)\n",
    "# (based on the H-parameters set above, for each splitted training set)\n",
    "WEIGHT_RIDGE = []\n",
    "LOSS_RIDGE = []\n",
    "\n",
    "for i in range(len(TX)):\n",
    "    best_deg, best_lambda = find_optimal(Y[i], TX[i], DEGREE, K_FOLD, LAMBDA, SEED)\n",
    "    print('The best degree of tx_{}:'.format(i), best_deg, 'with lambda:', best_lambda)\n",
    "    \n",
    "    poly_tx = build_poly(TX[i], best_deg)\n",
    "    w, loss = ridge_regression(Y[i], poly_tx, best_lambda)\n",
    "    \n",
    "    WEIGHT.append(w)\n",
    "    LOSS_RIDGE.append(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression (SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set H-parameters\n",
    "MAX_ITERS_LOGIC = 500\n",
    "THRESHOLD_LOGIC = 1e-8\n",
    "GAMMA_LOGIC = 0.005\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape label y to 2D-array\n",
    "Y_LOGIC = []\n",
    "\n",
    "for i in range(len(Y)):\n",
    "    Y_LOGIC.append(np.array(Y[i]).reshape(-1, 1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WEIGHT_LOGIC = []\n",
    "#LOSS_LOGIC = []\n",
    "\n",
    "INITIAL_W = np.zeros((TX[0].shape[1]+1, 1))\n",
    "w_rlogic_0, loss_rlogic_0 = logistic_regression(Y_LOGIC[0], TX[0], INITIAL_W, MAX_ITERS_LOGIC,\n",
    "                                                GAMMA_LOGIC, THRESHOLD_LOGIC, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INITIAL_W = np.zeros((TX[1].shape[1]+1, 1))\n",
    "w_rlogic_1, loss_rlogic_1 = logistic_regression(Y_LOGIC[1], TX[1], INITIAL_W, MAX_ITERS_LOGIC,\n",
    "                                                GAMMA_LOGIC, THRESHOLD_LOGIC, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INITIAL_W = np.zeros((TX[2].shape[1]+1, 1))\n",
    "w_rlogic_2, loss_rlogic_2 = logistic_regression(Y_LOGIC[2], TX[2], INITIAL_W, MAX_ITERS_LOGIC,\n",
    "                                                GAMMA_LOGIC, THRESHOLD_LOGIC, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized Logistic Regression (GD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set H-parameters\n",
    "MAX_ITERS_RLOGIC = 1000\n",
    "THRESHOLD_RLOGIC = 1e-8\n",
    "GAMMA_RLOGIC = 0.01\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape label y to 2D-array\n",
    "Y_REG_LOGIC = []\n",
    "\n",
    "for i in range(len(Y)):\n",
    "    Y_REG_LOGIC.append(np.array(Y[i]).reshape(-1, 1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning each splitted dataset\n",
    "INITIAL_W = np.zeros((TX[0].shape[1], 1))\n",
    "w_rlogic, loss_rlogic = reg_logistic_regression(Y_REG_LOGIC[0], TX[0], INITIAL_W, MAX_ITERS_RLOGIC,\n",
    "                                                GAMMA_RLOGIC, THRESHOLD_RLOGIC, BATCH_SIZE)\n",
    "    \n",
    "WEIGHT_RLOGIC.append(w)\n",
    "LOSS_RLOGIC.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INITIAL_W = np.zeros((TX[1].shape[1], 1))\n",
    "w_rlogic, loss_rlogic = reg_logistic_regression(Y_REG_LOGIC[1], TX[1], INITIAL_W, MAX_ITERS_RLOGIC,\n",
    "                                                GAMMA_RLOGIC, THRESHOLD_RLOGIC, BATCH_SIZE)\n",
    "    \n",
    "WEIGHT_RLOGIC.append(w)\n",
    "LOSS_RLOGIC.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INITIAL_W = np.zeros((TX[2].shape[1], 1))\n",
    "w_rlogic, loss_rlogic = reg_logistic_regression(Y_REG_LOGIC[2], TX[2], INITIAL_W, MAX_ITERS_RLOGIC,\n",
    "                                                GAMMA_RLOGIC, THRESHOLD_RLOGIC, BATCH_SIZE)\n",
    "    \n",
    "WEIGHT_RLOGIC.append(w)\n",
    "LOSS_RLOGIC.append(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = 'data/test.csv' # TODO: download train data and supply path here \n",
    "Y_test, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(227458, 18)\n",
      "(175338, 22)\n",
      "(165442, 29)\n"
     ]
    }
   ],
   "source": [
    "TX_test, _, r_ids_test = data_preprocess(tX_test, Y_test, ids_test, replacing='lr',\n",
    "                                     mode='std_norm')\n",
    "\n",
    "print(TX_test[0].shape)\n",
    "print(TX_test[1].shape)\n",
    "print(TX_test[2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# W_tr = []\n",
    "OUTPUT_PATH = 'data/pred.csv' # TODO: fill in desired name of output file for submission\n",
    "\n",
    "y_pred = data_pred(TX, W_tr, step='te')\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
