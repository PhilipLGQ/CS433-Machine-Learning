{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import files to use in preprocessing and machine learning\n",
    "from implementations import *\n",
    "from proj1_helpers import *\n",
    "from preprocess import *\n",
    "from cross_validation import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Download train data and supply path here \n",
    "DATA_TRAIN_PATH = '../data/train.csv' \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000,)\n",
      "(250000, 30)\n",
      "(250000,)\n"
     ]
    }
   ],
   "source": [
    "# Check the array shape of y, tX, and ids\n",
    "print(y.shape)\n",
    "print(tX.shape)\n",
    "print(ids.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In observing the original training data, we found out that there exists missing data all over tX. The missing data are represented as value -999. Considering that these columns are critical in model training, we cannot simply delete these rows with -999 values. Therefore, we need to process the original training set before model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we check the columns of tX to obtain an overview of missing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 0 has 15.2456% percentage of missing values\n",
      "P(y = 1|x having -999) = 7.438%\n",
      "P(y = -1|x having -999) = 92.562% \n",
      "\n",
      "Column 4 has 70.9828% percentage of missing values\n",
      "P(y = 1|x having -999) = 29.980%\n",
      "P(y = -1|x having -999) = 70.020% \n",
      "\n",
      "Column 5 has 70.9828% percentage of missing values\n",
      "P(y = 1|x having -999) = 29.980%\n",
      "P(y = -1|x having -999) = 70.020% \n",
      "\n",
      "Column 6 has 70.9828% percentage of missing values\n",
      "P(y = 1|x having -999) = 29.980%\n",
      "P(y = -1|x having -999) = 70.020% \n",
      "\n",
      "Column 12 has 70.9828% percentage of missing values\n",
      "P(y = 1|x having -999) = 29.980%\n",
      "P(y = -1|x having -999) = 70.020% \n",
      "\n",
      "Column 23 has 39.9652% percentage of missing values\n",
      "P(y = 1|x having -999) = 25.514%\n",
      "P(y = -1|x having -999) = 74.486% \n",
      "\n",
      "Column 24 has 39.9652% percentage of missing values\n",
      "P(y = 1|x having -999) = 25.514%\n",
      "P(y = -1|x having -999) = 74.486% \n",
      "\n",
      "Column 25 has 39.9652% percentage of missing values\n",
      "P(y = 1|x having -999) = 25.514%\n",
      "P(y = -1|x having -999) = 74.486% \n",
      "\n",
      "Column 26 has 70.9828% percentage of missing values\n",
      "P(y = 1|x having -999) = 29.980%\n",
      "P(y = -1|x having -999) = 70.020% \n",
      "\n",
      "Column 27 has 70.9828% percentage of missing values\n",
      "P(y = 1|x having -999) = 29.980%\n",
      "P(y = -1|x having -999) = 70.020% \n",
      "\n",
      "Column 28 has 70.9828% percentage of missing values\n",
      "P(y = 1|x having -999) = 29.980%\n",
      "P(y = -1|x having -999) = 70.020% \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check whether the missing values are associated with the classification result\n",
    "for col in range(tX.shape[1]):\n",
    "    tX_T = np.transpose(tX)\n",
    "    \n",
    "    null = (tX_T[col] == -999)\n",
    "    null_s = np.logical_and(y >= 0, null)\n",
    "    null_b = np.logical_and(y < 0, null)\n",
    "    \n",
    "    tX_null = tX[null]\n",
    "    tX_null_s = tX[null_s]\n",
    "    tX_null_b = tX[null_b]\n",
    "    \n",
    "    if (tX_null.shape[0] > 0):\n",
    "        # Print the percentage of column 'col' having a -999 (missing) value\n",
    "        print('Column', col, 'has {}% percentage of missing values'.format(tX_null.shape[0] * 100 / tX.shape[0]))\n",
    "\n",
    "        # Print the conditional probability of P(y = 1|x having -999)\n",
    "        print('P(y = 1|x having -999) = {:.3f}%'.format(tX_null_s.shape[0] * 100 / tX_null.shape[0]))\n",
    "        \n",
    "        # Print the conditional probability of P(y = -1|x having -999)\n",
    "        print('P(y = -1|x having -999) = {:.3f}% \\n'.format(tX_null_b.shape[0] * 100 / tX_null.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that 11 columns contains at least one -999 (missing value). Now we check whether some of the missing values are dependent on the column named 'PRI_jet_num' (column No. 23), since 'PRI_jet_num' has a discrete value range {0, 1, 2, 3} and our observation on the beginning data rows showed a dependency of some missing values to the value of 'PRI_jet_num' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of rows for different PRI_jet_num: 250000 \n",
      "\n",
      "PRI_jet_num = 0 No. of columns having -999 (a missing value):[10, 11]\n",
      "PRI_jet_num = 1 No. of columns having -999 (a missing value):[8, 7]\n",
      "PRI_jet_num = 2 No. of columns having -999 (a missing value):[0, 1]\n",
      "PRI_jet_num = 3 No. of columns having -999 (a missing value):[0, 1]\n"
     ]
    }
   ],
   "source": [
    "PRI_jet_range = [i for i in range(0, 4)]\n",
    "PRI_jet_sum = []\n",
    "PRI_jet_null = []\n",
    "\n",
    "for value in PRI_jet_range:\n",
    "    tX_PRI = tX[tX[:, 22] == value]\n",
    "    \n",
    "    # Append values of row numbers for different PRI_jet_num, finally sum up to see whether it equals to the length of tX\n",
    "    PRI_jet_sum.append(len(tX_PRI))\n",
    "    \n",
    "    # Count the number of missing columns corresponding to different PRI_jet_num values\n",
    "    PRI_jet_keys = []\n",
    "    for i in range (len(tX_PRI)):\n",
    "        tX_null_cols = np.count_nonzero(tX_PRI[i] == -999, axis = 0)\n",
    "        PRI_jet_keys.append(tX_null_cols)\n",
    "    \n",
    "    PRI_jet_null.append(list(set(PRI_jet_keys)))\n",
    "\n",
    "    \n",
    "print(\"Sum of rows for different PRI_jet_num: {} \\n\".format(sum(PRI_jet_sum)))\n",
    "\n",
    "for i in range(4):\n",
    "    print(\"PRI_jet_num =\", PRI_jet_range[i], \"No. of columns having -999 (a missing value):{}\".format(PRI_jet_null[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above analysis showed that one column with -999 (missing value) is independent of the column 'PRI_jet_num', we check the original training set and we can easily find out that the first tX column 'DER_mass_MMC' is independent of 'PRI_jet_num'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the data analysis above, we conduct the following method to pre-process the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set H-parameters\n",
    "K_FOLD = 10\n",
    "DEGREE = np.arange(1, 8)\n",
    "SEED = 5\n",
    "LAMBDA = np.logspace(-6, -2, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal(x, y, degrees, k_fold, lambdas, seed=1):\n",
    "    # Split the data into k-fold\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    \n",
    "    # Set lists for collecting best lambda & rmse for each degree\n",
    "    best_lambda = []\n",
    "    best_rmse = []\n",
    "    \n",
    "    for degree in degrees:\n",
    "        rmse_val = []\n",
    "        \n",
    "        for lambda_ in lambdas:\n",
    "            rmse_val_lambda_ = []\n",
    "            \n",
    "            for k in range(k_fold):\n",
    "                _, loss_val, w = cross_validation(y, x, k_indices, k, lambda_, degree)\n",
    "                rmse_val_lambda_.append(loss_val)\n",
    "                \n",
    "            print(\"lambda {}\".format(lambda_))\n",
    "            print(\"loss {}\".format(np.mean(rmse_val_lambda_)))\n",
    "            print(\"degree {}\".format(degree))\n",
    "            print(\"\\n\\n\")\n",
    "\n",
    "            rmse_val.append(np.mean(rmse_val_lambda_))\n",
    "        \n",
    "        index_opt_lambda = np.argmin(rmse_val)\n",
    "        best_lambda.append(lambdas[index_opt_lambda])\n",
    "        best_rmse.append(rmse_val[index_opt_lambda])\n",
    "    \n",
    "    opt_degree = degrees[np.argmin(best_rmse)]\n",
    "    opt_lambda = best_lambda[np.argmin(best_rmse)]\n",
    "    \n",
    "    return opt_degree, opt_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(feature, label, degrees, k_fold, lambdas):\n",
    "    opt_degree, opt_lambda = [], []\n",
    "    \n",
    "    feature_arr = split_reformat_feature(feature)\n",
    "    label_arr = split_label(feature, label)\n",
    "    \n",
    "    # Parallel iteration for cross validation to select the best degree of complexity and learning rate\n",
    "    # Train\n",
    "    for f, l in zip(feature_arr, label_arr):\n",
    "        opt_d, opt_l = find_optimal(f, l, degrees, k_fold, lambdas)\n",
    "        opt_degree.append(opt_d)\n",
    "        opt_lambda.append(opt_l)\n",
    "        \n",
    "    return opt_degree, opt_lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda 1e-06\n",
      "loss 0.8718895832839373\n",
      "degree 1\n",
      "\n",
      "\n",
      "\n",
      "lambda 1.3738237958832638e-06\n",
      "loss 0.8718895832886332\n",
      "degree 1\n",
      "\n",
      "\n",
      "\n",
      "lambda 1.8873918221350957e-06\n",
      "loss 0.8718895832953357\n",
      "degree 1\n",
      "\n",
      "\n",
      "\n",
      "lambda 2.592943797404667e-06\n",
      "loss 0.8718895833050165\n",
      "degree 1\n",
      "\n",
      "\n",
      "\n",
      "lambda 3.562247890262444e-06\n",
      "loss 0.8718895833192093\n",
      "degree 1\n",
      "\n",
      "\n",
      "\n",
      "lambda 4.893900918477499e-06\n",
      "loss 0.8718895833403929\n",
      "degree 1\n",
      "\n",
      "\n",
      "\n",
      "lambda 6.723357536499335e-06\n",
      "loss 0.8718895833726762\n",
      "degree 1\n",
      "\n",
      "\n",
      "\n",
      "lambda 9.236708571873865e-06\n",
      "loss 0.8718895834230311\n",
      "degree 1\n",
      "\n",
      "\n",
      "\n",
      "lambda 1.2689610031679234e-05\n",
      "loss 0.8718895835035397\n",
      "degree 1\n",
      "\n",
      "\n",
      "\n",
      "lambda 1.7433288221999873e-05\n",
      "loss 0.8718895836355284\n",
      "degree 1\n",
      "\n",
      "\n",
      "\n",
      "lambda 2.395026619987486e-05\n",
      "loss 0.8718895838572136\n",
      "degree 1\n",
      "\n",
      "\n",
      "\n",
      "lambda 3.290344562312671e-05\n",
      "loss 0.8718895842379328\n",
      "degree 1\n",
      "\n",
      "\n",
      "\n",
      "lambda 4.520353656360241e-05\n",
      "loss 0.8718895849047067\n",
      "degree 1\n",
      "\n",
      "\n",
      "\n",
      "lambda 6.210169418915616e-05\n",
      "loss 0.871889586091975\n",
      "degree 1\n",
      "\n",
      "\n",
      "\n",
      "lambda 8.531678524172815e-05\n",
      "loss 0.8718895882348981\n",
      "degree 1\n",
      "\n",
      "\n",
      "\n",
      "lambda 0.00011721022975334806\n",
      "loss 0.8718895921446348\n",
      "degree 1\n",
      "\n",
      "\n",
      "\n",
      "lambda 0.00016102620275609394\n",
      "loss 0.8718895993379288\n",
      "degree 1\n",
      "\n",
      "\n",
      "\n",
      "lambda 0.0002212216291070448\n",
      "loss 0.8718896126572142\n",
      "degree 1\n",
      "\n",
      "\n",
      "\n",
      "lambda 0.0003039195382313198\n",
      "loss 0.8718896374376268\n",
      "degree 1\n",
      "\n",
      "\n",
      "\n",
      "lambda 0.00041753189365604\n",
      "loss 0.8718896837031892\n",
      "degree 1\n",
      "\n",
      "\n",
      "\n",
      "lambda 0.0005736152510448681\n",
      "loss 0.8718897702984527\n",
      "degree 1\n",
      "\n",
      "\n",
      "\n",
      "lambda 0.0007880462815669912\n",
      "loss 0.8718899326561592\n",
      "degree 1\n",
      "\n",
      "\n",
      "\n",
      "lambda 0.001082636733874054\n",
      "loss 0.8718902373853809\n",
      "degree 1\n",
      "\n",
      "\n",
      "\n",
      "lambda 0.0014873521072935117\n",
      "loss 0.8718908096261672\n",
      "degree 1\n",
      "\n",
      "\n",
      "\n",
      "lambda 0.0020433597178569417\n",
      "loss 0.8718918842287122\n",
      "degree 1\n",
      "\n",
      "\n",
      "\n",
      "lambda 0.0028072162039411755\n",
      "loss 0.8718939012074918\n",
      "degree 1\n",
      "\n",
      "\n",
      "\n",
      "lambda 0.0038566204211634724\n",
      "loss 0.8718976829973016\n",
      "degree 1\n",
      "\n",
      "\n",
      "\n",
      "lambda 0.005298316906283708\n",
      "loss 0.8719047616249123\n",
      "degree 1\n",
      "\n",
      "\n",
      "\n",
      "lambda 0.007278953843983146\n",
      "loss 0.8719179775298954\n",
      "degree 1\n",
      "\n",
      "\n",
      "\n",
      "lambda 0.01\n",
      "loss 0.8719425628337751\n",
      "degree 1\n",
      "\n",
      "\n",
      "\n",
      "lambda 1e-06\n",
      "loss 0.7380427830531685\n",
      "degree 2\n",
      "\n",
      "\n",
      "\n",
      "lambda 1.3738237958832638e-06\n",
      "loss 0.7380427662213969\n",
      "degree 2\n",
      "\n",
      "\n",
      "\n",
      "lambda 1.8873918221350957e-06\n",
      "loss 0.7380427432065527\n",
      "degree 2\n",
      "\n",
      "\n",
      "\n",
      "lambda 2.592943797404667e-06\n",
      "loss 0.738042711676666\n",
      "degree 2\n",
      "\n",
      "\n",
      "\n",
      "lambda 3.562247890262444e-06\n",
      "loss 0.7380426684416725\n",
      "degree 2\n",
      "\n",
      "\n",
      "\n",
      "lambda 4.893900918477499e-06\n",
      "loss 0.7380426091360642\n",
      "degree 2\n",
      "\n",
      "\n",
      "\n",
      "lambda 6.723357536499335e-06\n",
      "loss 0.7380425277883053\n",
      "degree 2\n",
      "\n",
      "\n",
      "\n",
      "lambda 9.236708571873865e-06\n",
      "loss 0.7380424162388204\n",
      "degree 2\n",
      "\n",
      "\n",
      "\n",
      "lambda 1.2689610031679234e-05\n",
      "loss 0.7380422633579216\n",
      "degree 2\n",
      "\n",
      "\n",
      "\n",
      "lambda 1.7433288221999873e-05\n",
      "loss 0.7380420540040235\n",
      "degree 2\n",
      "\n",
      "\n",
      "\n",
      "lambda 2.395026619987486e-05\n",
      "loss 0.7380417676536319\n",
      "degree 2\n",
      "\n",
      "\n",
      "\n",
      "lambda 3.290344562312671e-05\n",
      "loss 0.7380413766341605\n",
      "degree 2\n",
      "\n",
      "\n",
      "\n",
      "lambda 4.520353656360241e-05\n",
      "loss 0.7380408439119919\n",
      "degree 2\n",
      "\n",
      "\n",
      "\n",
      "lambda 6.210169418915616e-05\n",
      "loss 0.738040120458386\n",
      "degree 2\n",
      "\n",
      "\n",
      "\n",
      "lambda 8.531678524172815e-05\n",
      "loss 0.7380391423878159\n",
      "degree 2\n",
      "\n",
      "\n",
      "\n",
      "lambda 0.00011721022975334806\n",
      "loss 0.7380378284382985\n",
      "degree 2\n",
      "\n",
      "\n",
      "\n",
      "lambda 0.00016102620275609394\n",
      "loss 0.7380360791303147\n",
      "degree 2\n",
      "\n",
      "\n",
      "\n",
      "lambda 0.0002212216291070448\n",
      "loss 0.7380337804450676\n",
      "degree 2\n",
      "\n",
      "\n",
      "\n",
      "lambda 0.0003039195382313198\n",
      "loss 0.7380308177168252\n",
      "degree 2\n",
      "\n",
      "\n",
      "\n",
      "lambda 0.00041753189365604\n",
      "loss 0.7380271106877403\n",
      "degree 2\n",
      "\n",
      "\n",
      "\n",
      "lambda 0.0005736152510448681\n",
      "loss 0.7380226900359822\n",
      "degree 2\n",
      "\n",
      "\n",
      "\n",
      "lambda 0.0007880462815669912\n",
      "loss 0.7380178517301212\n",
      "degree 2\n",
      "\n",
      "\n",
      "\n",
      "lambda 0.001082636733874054\n",
      "loss 0.7380134516494066\n",
      "degree 2\n",
      "\n",
      "\n",
      "\n",
      "lambda 0.0014873521072935117\n",
      "loss 0.7380114422442448\n",
      "degree 2\n",
      "\n",
      "\n",
      "\n",
      "lambda 0.0020433597178569417\n",
      "loss 0.7380158056148296\n",
      "degree 2\n",
      "\n",
      "\n",
      "\n",
      "lambda 0.0028072162039411755\n",
      "loss 0.7380340934870345\n",
      "degree 2\n",
      "\n",
      "\n",
      "\n",
      "lambda 0.0038566204211634724\n",
      "loss 0.7380798141984248\n",
      "degree 2\n",
      "\n",
      "\n",
      "\n",
      "lambda 0.005298316906283708\n",
      "loss 0.7381758510862172\n",
      "degree 2\n",
      "\n",
      "\n",
      "\n",
      "lambda 0.007278953843983146\n",
      "loss 0.7383588768783735\n",
      "degree 2\n",
      "\n",
      "\n",
      "\n",
      "lambda 0.01\n",
      "loss 0.7386842943495019\n",
      "degree 2\n",
      "\n",
      "\n",
      "\n",
      "lambda 1e-06\n",
      "loss 1.3535962539207758\n",
      "degree 3\n",
      "\n",
      "\n",
      "\n",
      "lambda 1.3738237958832638e-06\n",
      "loss 1.3535929429394407\n",
      "degree 3\n",
      "\n",
      "\n",
      "\n",
      "lambda 1.8873918221350957e-06\n",
      "loss 1.3535883991919164\n",
      "degree 3\n",
      "\n",
      "\n",
      "\n",
      "lambda 2.592943797404667e-06\n",
      "loss 1.3535821603856313\n",
      "degree 3\n",
      "\n",
      "\n",
      "\n",
      "lambda 3.562247890262444e-06\n",
      "loss 1.353573591713547\n",
      "degree 3\n",
      "\n",
      "\n",
      "\n",
      "lambda 4.893900918477499e-06\n",
      "loss 1.3535618211985925\n",
      "degree 3\n",
      "\n",
      "\n",
      "\n",
      "lambda 6.723357536499335e-06\n",
      "loss 1.3535456508394135\n",
      "degree 3\n",
      "\n",
      "\n",
      "\n",
      "lambda 9.236708571873865e-06\n",
      "loss 1.353523434454273\n",
      "degree 3\n",
      "\n",
      "\n",
      "\n",
      "lambda 1.2689610031679234e-05\n",
      "loss 1.353492909675425\n",
      "degree 3\n",
      "\n",
      "\n",
      "\n",
      "lambda 1.7433288221999873e-05\n",
      "loss 1.3534509667757582\n",
      "degree 3\n",
      "\n",
      "\n",
      "\n",
      "lambda 2.395026619987486e-05\n",
      "loss 1.3533933303869645\n",
      "degree 3\n",
      "\n",
      "\n",
      "\n",
      "lambda 3.290344562312671e-05\n",
      "loss 1.353314120941329\n",
      "degree 3\n",
      "\n",
      "\n",
      "\n",
      "lambda 4.520353656360241e-05\n",
      "loss 1.353205249765183\n",
      "degree 3\n",
      "\n",
      "\n",
      "\n",
      "lambda 6.210169418915616e-05\n",
      "loss 1.353055583616429\n",
      "degree 3\n",
      "\n",
      "\n",
      "\n",
      "lambda 8.531678524172815e-05\n",
      "loss 1.3528497888434419\n",
      "degree 3\n",
      "\n",
      "\n",
      "\n",
      "lambda 0.00011721022975334806\n",
      "loss 1.352566728994607\n",
      "degree 3\n",
      "\n",
      "\n",
      "\n",
      "lambda 0.00016102620275609394\n",
      "loss 1.3521772379957873\n",
      "degree 3\n",
      "\n",
      "\n",
      "\n",
      "lambda 0.0002212216291070448\n",
      "loss 1.3516410175955822\n",
      "degree 3\n",
      "\n",
      "\n",
      "\n",
      "lambda 0.0003039195382313198\n",
      "loss 1.350902304731075\n",
      "degree 3\n",
      "\n",
      "\n",
      "\n",
      "lambda 0.00041753189365604\n",
      "loss 1.349883813919204\n",
      "degree 3\n",
      "\n",
      "\n",
      "\n",
      "lambda 0.0005736152510448681\n",
      "loss 1.3484782794682453\n",
      "degree 3\n",
      "\n",
      "\n",
      "\n",
      "lambda 0.0007880462815669912\n",
      "loss 1.3465367194257092\n",
      "degree 3\n",
      "\n",
      "\n",
      "\n",
      "lambda 0.001082636733874054\n",
      "loss 1.3438523823847306\n",
      "degree 3\n",
      "\n",
      "\n",
      "\n",
      "lambda 0.0014873521072935117\n",
      "loss 1.3401393755273543\n",
      "degree 3\n",
      "\n",
      "\n",
      "\n",
      "lambda 0.0020433597178569417\n",
      "loss 1.3350054956009445\n",
      "degree 3\n",
      "\n",
      "\n",
      "\n",
      "lambda 0.0028072162039411755\n",
      "loss 1.3279201850944704\n",
      "degree 3\n",
      "\n",
      "\n",
      "\n",
      "lambda 0.0038566204211634724\n",
      "loss 1.3181810919825279\n",
      "degree 3\n",
      "\n",
      "\n",
      "\n",
      "lambda 0.005298316906283708\n",
      "loss 1.3048861203981474\n",
      "degree 3\n",
      "\n",
      "\n",
      "\n",
      "lambda 0.007278953843983146\n",
      "loss 1.286920759573937\n",
      "degree 3\n",
      "\n",
      "\n",
      "\n",
      "lambda 0.01\n",
      "loss 1.2629706750277307\n",
      "degree 3\n",
      "\n",
      "\n",
      "\n",
      "lambda 1e-06\n",
      "loss 2.7228649627781474\n",
      "degree 4\n",
      "\n",
      "\n",
      "\n",
      "lambda 1.3738237958832638e-06\n",
      "loss 2.7242536306340033\n",
      "degree 4\n",
      "\n",
      "\n",
      "\n",
      "lambda 1.8873918221350957e-06\n",
      "loss 2.7261621001309537\n",
      "degree 4\n",
      "\n",
      "\n",
      "\n",
      "lambda 2.592943797404667e-06\n",
      "loss 2.7287844240444223\n",
      "degree 4\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/0m/zxq3kg3515dgv1fvq2yrlzdw0000gn/T/ipykernel_30745/2116683607.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbest_degree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_lambda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEGREE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK_FOLD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLAMBDA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/0m/zxq3kg3515dgv1fvq2yrlzdw0000gn/T/ipykernel_30745/2655537036.py\u001b[0m in \u001b[0;36mtrain_models\u001b[0;34m(feature, label, degrees, k_fold, lambdas)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_arr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mopt_d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_l\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_optimal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdegrees\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_fold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambdas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mopt_degree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt_d\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mopt_lambda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt_l\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/0m/zxq3kg3515dgv1fvq2yrlzdw0000gn/T/ipykernel_30745/3878876902.py\u001b[0m in \u001b[0;36mfind_optimal\u001b[0;34m(x, y, degrees, k_fold, lambdas, seed)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_fold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m                 \u001b[0mrmse_val_lambda_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/CS433-Machine-Learning/projects/project1/scripts/cross_validation.py\u001b[0m in \u001b[0;36mcross_validation\u001b[0;34m(y, x, k_indices, k, lambda_, degree)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m# Form data with polynomial degree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mpoly_tx_tr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_poly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mpoly_tx_te\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_poly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_te\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/CS433-Machine-Learning/projects/project1/scripts/helpers.py\u001b[0m in \u001b[0;36mbuild_poly\u001b[0;34m(x, degree)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mpoly\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdegrees\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mpoly\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpoly\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdegrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpoly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_degree, best_lambda = train_models(tX, y, DEGREE, K_FOLD, LAMBDA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# OUTPUT_PATH = 'data/pred.csv' # TODO: fill in desired name of output file for submission\n",
    "# y_pred = predict_labels(weights, tX_test)\n",
    "# create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(227458, 18)\n",
      "(175338, 22)\n",
      "(165442, 29)\n"
     ]
    }
   ],
   "source": [
    "t0, t1, t23, t_ids = split_reformat_test(tX_test, ids_test)\n",
    "\n",
    "\n",
    "print(t0.shape)\n",
    "print(t1.shape)\n",
    "print(t23.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_t1 = build_poly(t1, opt_d)\n",
    "\n",
    "\n",
    "p = predict_labels(quanzhong, poly_t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1306904378970902"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(p[p == 1]) / len(p) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35734550706695556"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_jet_1[y_jet_1 == 1]) / len(y_jet_1) "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
