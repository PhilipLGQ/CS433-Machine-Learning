{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import files to use in preprocessing and machine learning\n",
    "from implementations import *\n",
    "from proj1_helpers import *\n",
    "from preprocess import *\n",
    "from cross_validation import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Download train data and supply path here \n",
    "DATA_TRAIN_PATH = 'data_aicrowd/train.csv' \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000,)\n",
      "(250000, 30)\n",
      "(250000,)\n"
     ]
    }
   ],
   "source": [
    "# Check the array shape of y, tX, and ids\n",
    "print(y.shape)\n",
    "print(tX.shape)\n",
    "print(ids.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In observing the original training data, we found out that there exists missing data all over tX. The missing data are represented as value -999. Considering that these columns are critical in model training, we cannot simply delete these rows with -999 values. Therefore, we need to process the original training set before model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we check the columns of tX to obtain an overview of missing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 0 has 15.2456% percentage of missing values\n",
      "P(y = 1|x having -999) = 7.438%\n",
      "P(y = -1|x having -999) = 92.562% \n",
      "\n",
      "Column 4 has 70.9828% percentage of missing values\n",
      "P(y = 1|x having -999) = 29.980%\n",
      "P(y = -1|x having -999) = 70.020% \n",
      "\n",
      "Column 5 has 70.9828% percentage of missing values\n",
      "P(y = 1|x having -999) = 29.980%\n",
      "P(y = -1|x having -999) = 70.020% \n",
      "\n",
      "Column 6 has 70.9828% percentage of missing values\n",
      "P(y = 1|x having -999) = 29.980%\n",
      "P(y = -1|x having -999) = 70.020% \n",
      "\n",
      "Column 12 has 70.9828% percentage of missing values\n",
      "P(y = 1|x having -999) = 29.980%\n",
      "P(y = -1|x having -999) = 70.020% \n",
      "\n",
      "Column 23 has 39.9652% percentage of missing values\n",
      "P(y = 1|x having -999) = 25.514%\n",
      "P(y = -1|x having -999) = 74.486% \n",
      "\n",
      "Column 24 has 39.9652% percentage of missing values\n",
      "P(y = 1|x having -999) = 25.514%\n",
      "P(y = -1|x having -999) = 74.486% \n",
      "\n",
      "Column 25 has 39.9652% percentage of missing values\n",
      "P(y = 1|x having -999) = 25.514%\n",
      "P(y = -1|x having -999) = 74.486% \n",
      "\n",
      "Column 26 has 70.9828% percentage of missing values\n",
      "P(y = 1|x having -999) = 29.980%\n",
      "P(y = -1|x having -999) = 70.020% \n",
      "\n",
      "Column 27 has 70.9828% percentage of missing values\n",
      "P(y = 1|x having -999) = 29.980%\n",
      "P(y = -1|x having -999) = 70.020% \n",
      "\n",
      "Column 28 has 70.9828% percentage of missing values\n",
      "P(y = 1|x having -999) = 29.980%\n",
      "P(y = -1|x having -999) = 70.020% \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check whether the missing values are associated with the classification result\n",
    "for col in range(tX.shape[1]):\n",
    "    tX_T = np.transpose(tX)\n",
    "    \n",
    "    null = (tX_T[col] == -999)\n",
    "    null_s = np.logical_and(y >= 0, null)\n",
    "    null_b = np.logical_and(y < 0, null)\n",
    "    \n",
    "    tX_null = tX[null]\n",
    "    tX_null_s = tX[null_s]\n",
    "    tX_null_b = tX[null_b]\n",
    "    \n",
    "    if (tX_null.shape[0] > 0):\n",
    "        # Print the percentage of column 'col' having a -999 (missing) value\n",
    "        print('Column', col, 'has {}% percentage of missing values'.format(tX_null.shape[0] * 100 / tX.shape[0]))\n",
    "\n",
    "        # Print the conditional probability of P(y = 1|x having -999)\n",
    "        print('P(y = 1|x having -999) = {:.3f}%'.format(tX_null_s.shape[0] * 100 / tX_null.shape[0]))\n",
    "        \n",
    "        # Print the conditional probability of P(y = -1|x having -999)\n",
    "        print('P(y = -1|x having -999) = {:.3f}% \\n'.format(tX_null_b.shape[0] * 100 / tX_null.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that 11 columns contains at least one -999 (missing value). Now we check whether some of the missing values are dependent on the column named 'PRI_jet_num' (column No. 23), since 'PRI_jet_num' has a discrete value range {0, 1, 2, 3} and our observation on the beginning data rows showed a dependency of some missing values to the value of 'PRI_jet_num' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of rows for different PRI_jet_num: 250000 \n",
      "\n",
      "PRI_jet_num = 0 No. of columns having -999 (a missing value):[10, 11]\n",
      "PRI_jet_num = 1 No. of columns having -999 (a missing value):[8, 7]\n",
      "PRI_jet_num = 2 No. of columns having -999 (a missing value):[0, 1]\n",
      "PRI_jet_num = 3 No. of columns having -999 (a missing value):[0, 1]\n"
     ]
    }
   ],
   "source": [
    "PRI_jet_range = [i for i in range(0, 4)]\n",
    "PRI_jet_sum = []\n",
    "PRI_jet_null = []\n",
    "\n",
    "for value in PRI_jet_range:\n",
    "    tX_PRI = tX[tX[:, 22] == value]\n",
    "    \n",
    "    # Append values of row numbers for different PRI_jet_num, finally sum up to see whether it equals to the length of tX\n",
    "    PRI_jet_sum.append(len(tX_PRI))\n",
    "    \n",
    "    # Count the number of missing columns corresponding to different PRI_jet_num values\n",
    "    PRI_jet_keys = []\n",
    "    for i in range (len(tX_PRI)):\n",
    "        tX_null_cols = np.count_nonzero(tX_PRI[i] == -999, axis = 0)\n",
    "        PRI_jet_keys.append(tX_null_cols)\n",
    "    \n",
    "    PRI_jet_null.append(list(set(PRI_jet_keys)))\n",
    "\n",
    "    \n",
    "print(\"Sum of rows for different PRI_jet_num: {} \\n\".format(sum(PRI_jet_sum)))\n",
    "\n",
    "for i in range(4):\n",
    "    print(\"PRI_jet_num =\", PRI_jet_range[i], \"No. of columns having -999 (a missing value):{}\".format(PRI_jet_null[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above analysis showed that one column with -999 (missing value) is independent of the column 'PRI_jet_num', we check the original training set and we can easily find out that the first tX column 'DER_mass_MMC' is independent of 'PRI_jet_num'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the data analysis above, we conduct the following method to pre-process the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99913, 18)\n",
      "(77544, 22)\n",
      "(72543, 29)\n"
     ]
    }
   ],
   "source": [
    "# Split the database based on 'PRI_jet_num' (0, 1, 2&3)\n",
    "tX_jet_0, tX_jet_1, tX_jet_23, y_jet_0, y_jet_1, y_jet_23, r_ids = split_reformat_data(tX, y, ids)\n",
    "\n",
    "print(tX_jet_0.shape)\n",
    "print(tX_jet_1.shape)\n",
    "print(tX_jet_23.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing the missing values of first feature column with median\n",
    "replace_missing_value(tX_jet_0, 0, 'median')\n",
    "replace_missing_value(tX_jet_1, 0, 'median')\n",
    "replace_missing_value(tX_jet_23, 0, 'median')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the splitted training data\n",
    "tx_0 = standardize(tX_jet_0)\n",
    "tx_1 = standardize(tX_jet_1)\n",
    "tx_23 = standardize(tX_jet_23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set H-parameters\n",
    "K_FOLD = 10\n",
    "DEGREE = np.arange(2, 11)\n",
    "SEED = 5\n",
    "LAMBDA = np.logspace(-8, -6, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal(degrees, k_fold, lambdas, seed=1):\n",
    "    # Split the data into k-fold\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    \n",
    "    # Set lists for collecting best lambda & rmse for each degree\n",
    "    best_lambda = []\n",
    "    best_rmse = []\n",
    "    \n",
    "    for degree in degrees:\n",
    "        rmse_val = []\n",
    "        \n",
    "        for lambda_ in lambdas:\n",
    "            rmse_val_lambda_ = []\n",
    "            \n",
    "            for k in range(k_fold):\n",
    "                _, loss_val, w = cross_validation(y, x, k_indices, k, lambda_, degree)\n",
    "                rmse_val_lambda_.append(loss_val)\n",
    "\n",
    "            rmse_val.append(np.mean(rmse_val_lambda_))\n",
    "        \n",
    "        index_opt_lambda = np.argmin(rmse_val)\n",
    "        best_lambda.append(lambdas[index_opt_lambda])\n",
    "        best_rmse.append(rmse_val[index_opt_lambda])\n",
    "    \n",
    "    opt_degree = degrees[np.argmin(best_rmse)]\n",
    "    opt_lambda = best_lambda[np.argmin(best_rmse)]\n",
    "    \n",
    "    return opt_degree, opt_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = 'data_aicrowd/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'data_aicrowd/pred.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(weights, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
